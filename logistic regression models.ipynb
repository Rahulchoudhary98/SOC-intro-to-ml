{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbb18d2-0594-41dd-a138-d483464d81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv('spam.csv', encoding='ISO-8859-1') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3fdf47-2085-4184-9c58-c90ea93ebafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=df.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbfff8c-0848-40ee-bc48-bbb9f050a92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\AppData\\Local\\Temp\\ipykernel_19200\\3288895809.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.rename(columns={'v1':'label','v2':'message'},inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dataframe.rename(columns={'v1':'label','v2':'message'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddaaac99-70de-44b5-bf24-8b95fac7882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\AppData\\Local\\Temp\\ipykernel_19200\\1692912076.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['label'] = dataframe['label'].map({'ham': 0, 'spam': 1})\n"
     ]
    }
   ],
   "source": [
    "dataframe['label'] = dataframe['label'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a280bd01-0e8f-4dd5-8247-468eda8a8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer ,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cee15d8-feb9-43f0-80ef-df660fb8ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Go, until, jurong, point, ,, crazy, .., Avail...\n",
      "1                [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
      "2       [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
      "3       [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
      "4       [Nah, I, do, n't, think, he, goes, to, usf, ,,...\n",
      "                              ...                        \n",
      "5567    [This, is, the, 2nd, time, we, have, tried, 2,...\n",
      "5568     [Will, Ì_, b, going, to, esplanade, fr, home, ?]\n",
      "5569    [Pity, ,, *, was, in, mood, for, that, ., So, ...\n",
      "5570    [The, guy, did, some, bitching, but, I, acted,...\n",
      "5571                  [Rofl, ., Its, true, to, its, name]\n",
      "Name: tokens, Length: 5572, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\AppData\\Local\\Temp\\ipykernel_19200\\1657836277.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['tokens'] = dataframe['message'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')       # Primary tokenizer\n",
    "nltk.download('punkt_tab')  \n",
    "dataframe['tokens'] = dataframe['message'].apply(word_tokenize)\n",
    "print(dataframe['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803c7945-36ac-4ed6-935a-9b0080c45316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Rahul\\AppData\\Local\\Temp\\ipykernel_19200\\3843461603.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['filtered_tokens'] = dataframe['tokens'].apply(\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "dataframe['filtered_tokens'] = dataframe['tokens'].apply(\n",
    "    lambda tokens: [token.lower() for token in tokens if token.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44271572-b9e8-4645-b9a9-e76f73f73488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [go, jurong, point, ,, crazy, .., available, b...\n",
      "1                [ok, lar, ..., joking, wif, u, oni, ...]\n",
      "2       [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
      "3       [u, dun, say, early, hor, ..., u, c, already, ...\n",
      "4       [nah, n't, think, goes, usf, ,, lives, around,...\n",
      "                              ...                        \n",
      "5567    [2nd, time, tried, 2, contact, u., u, å£750, p...\n",
      "5568               [ì_, b, going, esplanade, fr, home, ?]\n",
      "5569           [pity, ,, *, mood, ., ..., suggestions, ?]\n",
      "5570    [guy, bitching, acted, like, 'd, interested, b...\n",
      "5571                                [rofl, ., true, name]\n",
      "Name: filtered_tokens, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['filtered_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e3975cc-588c-463f-b06c-cc7b3f5b942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer('english')\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85cd6153-4190-46cf-80a1-06f63e5b4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['stemmed_tokens'] = dataframe['filtered_tokens'].apply(\n",
    "    lambda tokens: [stemmer.stem(token) for token in tokens]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ebb422d-b602-4f56-915e-fb7827fec99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['lemmatized_tokens']=dataframe['filtered_tokens'].apply(\n",
    "    lambda tokens: [stemmer.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eae763d-9794-48f6-b4db-972a38595ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2vmodel = KeyedVectors.load_word2vec_format(\n",
    "    'C:/Users/Rahul/Downloads/GoogleNews-vectors-negative300.bin.gz',\n",
    "    binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab23a0d-2664-41f6-a1ce-ac66a54f805f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(model\u001b[38;5;241m.\u001b[39mvector_size)  \n\u001b[1;32m----> 7\u001b[0m dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemmed_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m tokens: get_average_vector(tokens, model))\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mD:\\New folder\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(model\u001b[38;5;241m.\u001b[39mvector_size)  \n\u001b[1;32m----> 7\u001b[0m dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemmed_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m tokens: get_average_vector(tokens, model))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def get_average_vector(tokens, model):\n",
    "    valid_vectors = [model[word] for word in tokens if word in model.key_to_index]\n",
    "    if valid_vectors:\n",
    "        return np.mean(valid_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  \n",
    "dataframe['vector'] = dataframe['stemmed_tokens'].apply(lambda tokens: get_average_vector(tokens, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e950048-2ec4-4a2f-a900-a7a12b5e5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.vstack(dataframe['vector'].values)  \n",
    "y = dataframe['label'].values              \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('X_train:',X_train)\n",
    "print('X_test:',X_test)\n",
    "print('y_train:',y_train)\n",
    "print('y_test:',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c4a97-cf89-4454-8af5-e39f421b5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clff= LogisticRegression(max_iter=1000)  \n",
    "clff.fit(X_train, y_train)\n",
    "y_pred = clff.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267afa9-a83d-43ba-8409-dfd455614d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message_class(model,w2vmodel, message):\n",
    "    import numpy as np\n",
    "    import re\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import SnowballStemmer\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokens = word_tokenize(message)\n",
    "    filtered = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "    valid_vectors = [w2vmodel[word] for word in filtered if word in w2vmodel.key_to_index]\n",
    "    if valid_vectors:\n",
    "        vector = np.mean(valid_vectors, axis=0).reshape(1, -1)\n",
    "    else:\n",
    "        vector = np.zeros(w2vmodel.vector_size).reshape(1, -1)\n",
    "\n",
    "    # Predict using the trained classifier\n",
    "    prediction = clff.predict(vector)[0]\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a227a-61b3-4dd6-b62b-a17ffd456bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Congratulations You've won a free ticket. Text WIN to 12345.\"\n",
    "print(predict_message_class(model=dataframe, w2vmodel=model, message=message))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce2e80-051c-4399-95f6-85ea9f37570f",
   "metadata": {},
   "source": [
    "# question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cee493-ddef-4eef-9996-49cb8efbfa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4abf8-e37b-4f46-bc19-0d116c985856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "import emoji\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "df = pd.read_csv('Tweets.csv.zip')  \n",
    "df1 = df[['text', 'airline_sentiment']]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = contractions.fix(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if not emoji.is_emoji(token)]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df1['clean_text'] = df1['text'].apply(preprocess_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3941d7-acb6-4b77-8b84-dcab3351b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2vmodel = KeyedVectors.load_word2vec_format(\n",
    "    'C:/Users/Rahul/Downloads/GoogleNews-vectors-negative300.bin.gz',\n",
    "    binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c439f-6798-4ac9-90fa-fd0dc026ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tweet_to_vec(tweet, model, dim=300):\n",
    "    words = tweet.split()\n",
    "    valid_words = [w for w in words if w in model]\n",
    "    if not valid_words:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean([model[w] for w in valid_words], axis=0)\n",
    "\n",
    "X = np.vstack(df1['clean_text'].apply(lambda x: tweet_to_vec(x, w2vmodel)).values)\n",
    "y = df1['airline_sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952ab55-231c-417a-81f5-4c6b9497e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc242d-fe4f-48e3-ba33-eb499cae834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671db692-2496-42fb-a778-48dccade864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet_sentiment(model, w2v_model, tweet):\n",
    "    clean = preprocess_tweet(tweet)\n",
    "    vec = tweet_to_vec(clean, w2v_model)\n",
    "    pred = model.predict([vec])[0]\n",
    "    return pred\n",
    "\n",
    "# Example \n",
    "tweet = \"I love flying with Delta, always a great experience!\"\n",
    "print(predict_tweet_sentiment(clf, w2vmodel, tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4e645-2c46-4f85-a31b-538e1b66f4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
